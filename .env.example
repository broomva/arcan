# Arcan Agent Runtime — Environment Variables
# Copy to .env and fill in your values. Never commit .env files.
#
#   cp env.example .env

# ─── Provider Selection ──────────────────────────────────────────
# Explicit provider: "anthropic", "openai", "ollama"
# If unset, auto-detects from available API keys (Anthropic → OpenAI → Mock)
# ARCAN_PROVIDER=anthropic

# ─── Anthropic (Claude) ─────────────────────────────────────────
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-sonnet-4-5-20250929
# ANTHROPIC_MAX_TOKENS=4096
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# ─── OpenAI (GPT-4o, o1, etc.) ──────────────────────────────────
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o
# OPENAI_MAX_TOKENS=4096
# OPENAI_BASE_URL=https://api.openai.com

# ─── Ollama (local open models) ─────────────────────────────────
# No API key needed for local Ollama.
# OLLAMA_MODEL=llama3.2
# OLLAMA_MAX_TOKENS=4096
# OLLAMA_BASE_URL=http://localhost:11434

# ─── Other OpenAI-Compatible Providers ──────────────────────────
# Use ARCAN_PROVIDER=openai with a custom base URL:
#
# Together AI:
#   OPENAI_API_KEY=your-together-key
#   OPENAI_BASE_URL=https://api.together.xyz
#   OPENAI_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo
#
# Groq:
#   OPENAI_API_KEY=your-groq-key
#   OPENAI_BASE_URL=https://api.groq.com/openai
#   OPENAI_MODEL=llama-3.3-70b-versatile
#
# vLLM / LM Studio:
#   OPENAI_BASE_URL=http://localhost:8000
#   OPENAI_MODEL=your-model-name

# ─── Arcan Server ───────────────────────────────────────────────
# ARCAN_PORT=3000
# ARCAN_DATA_DIR=.arcan
# ARCAN_MAX_ITERATIONS=24
# ARCAN_APPROVAL_TIMEOUT=300
